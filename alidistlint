#!/usr/bin/env python3

'''Lint alidist recipes using yamllint and shellcheck.'''

from argparse import ArgumentParser, FileType, Namespace
import io
import itertools
import json
import os.path
import re
from subprocess import run, PIPE
import sys
import tempfile
from typing import Any, BinaryIO, Callable, Iterable, NamedTuple

import cerberus
import yaml
from yaml.error import MarkedYAMLError, YAMLError


GCC_LEVELS: dict[str, str] = {
    'error': 'error',
    'warning': 'warning',
    'info': 'note',
    'style': 'note',
}

GITHUB_LEVELS: dict[str, str] = {
    'error': 'error',
    'warning': 'warning',
    'info': 'notice',
    'style': 'notice',
}

YAMLLINT_LINE_PATTERN: re.Pattern = re.compile(r'''
^  (?P<fname>   .+?   )    :   # file name, be non-greedy
   (?P<line>    \d+   )    :   # line number
   (?P<column>  \d+   )    :\s # column number
\[ (?P<level>   \w+   ) \] \s  # error level (error or warning)
   (?P<message> .+    )    \s  # free-form message from yamllint
\( (?P<code>    [^)]+ ) \) $   # symbolic error code from yamllint
''', re.VERBOSE)


ValidationErrors = list[str | dict[Any, 'ValidationErrors']]
ValidationErrorTree = str | dict[Any, ValidationErrors] | ValidationErrors
'''An error tree as produced by cerberus.'''

ObjectPath = tuple[str | int, ...]
'''Specifies where in a nested object a value is located as a series of keys.

For instance, A is at path (0, "a") in the object [{"a": A}].
'''

FileParts = dict[str, tuple[str, int, bytes | None]]
'''Map temporary file name to original file name and line offset.

For FileParts of YAML header data, also includes the content of the file part,
for direct processing. For FileParts of scripts, this is None instead.
'''


class Error(NamedTuple):
    '''A linter message.

    Instances should contain line and column numbers relative to the original
    input file, not relative to any FileParts that might have been used.
    '''
    level: str
    message: str
    file_name: str
    line: int
    column: int
    end_line: int | None = None
    end_column: int | None = None

    def format_gcc(self) -> str:
        '''Turn the Error into a string like a GCC error message.'''
        return (f'{self.file_name}:{self.line}:{self.column}: '
                f'{GCC_LEVELS[self.level]}: {self.message}')

    def format_github(self) -> str:
        '''Turn the Error into a string that GitHub understands.

        If printed from a GitHub Action, this will show the error messages in
        the Files view.
        '''
        end_line = '' if self.end_line is None else f',endLine={self.end_line}'
        end_column = '' if self.end_column is None else \
            f',endColumn={self.end_column}'
        return (f'::{GITHUB_LEVELS[self.level]} file={self.file_name}'
                f',line={self.line}{end_line}'
                f',col={self.column}{end_column}::{self.message}')


ERROR_FORMATTERS: dict[str, Callable[[Error], str]] = {
    'gcc': Error.format_gcc,
    'github': Error.format_github,
}


class MockMark(NamedTuple):
    '''Used if a key is not present in the YAML.'''
    line: int = 0
    column: int = 0


def split_files(temp_dir: str, input_files: Iterable[BinaryIO]) \
        -> tuple[FileParts, FileParts]:
    '''Split every given file into its YAML header and script part.'''
    header_parts: FileParts = {}
    script_parts: FileParts = {}
    for input_file in input_files:
        orig_basename = os.path.basename(input_file.name)
        recipe = input_file.read()
        # Get the first byte of the '---\n' line (excluding the prev newline).
        separator_position = recipe.find(b'\n---\n') + 1
        yaml_text = recipe[:separator_position]
        with open(f'{temp_dir}/{orig_basename}.head.yaml', 'wb') as headerf:
            headerf.write(yaml_text)
            header_parts[headerf.name] = input_file.name, 0, yaml_text
        with open(f'{temp_dir}/{orig_basename}.script.sh', 'wb') as scriptf:
            scriptf.write(recipe[separator_position + 4:])
            # Add 1 to line offset for the separator line.
            script_parts[scriptf.name] = \
                input_file.name, yaml_text.count(b'\n') + 1, None
    return header_parts, script_parts


def shellcheck(recipes: FileParts) -> Iterable[Error]:
    '''Run shellcheck on a recipe.'''
    cmd = 'shellcheck', '--format=json1', '--shell=bash', *recipes.keys()
    result = run(cmd, stdout=PIPE, text=True, check=False)
    for comment in json.loads(result.stdout)['comments']:
        orig_file_name, line_offset, _ = recipes[comment['file']]
        yield Error(
            comment['level'],
            f"{comment['message']} [SC{comment['code']}]",
            orig_file_name,
            comment['line'] + line_offset,
            comment['column'],
            comment['endLine'] + line_offset,
            comment['endColumn'],
        )


def yamllint(headers: FileParts) -> Iterable[Error]:
    '''Run yamllint on a recipe's YAML header.'''
    cmd = 'yamllint', '-f', 'parsable', '-d', json.dumps({
        # https://yamllint.readthedocs.io/en/stable/configuration.html
        'extends': 'default',
        'rules': {
            # Be more lenient on line length, e.g. for incremental_recipe.
            'line-length': {'max': 120},
            # YAML headers don't have a '---' line at the beginning.
            'document-start': 'disable',
            # YAML has a gotcha with automatic octal numbers.
            'octal-values': {
                # Numbers starting with 0 are octal. This is usually a mistake.
                'forbid-implicit-octal': True,
                # Numbers starting with 0o are OK.
                'forbid-explicit-octal': False,
            },
        },
    }), *headers.keys()
    result = run(cmd, stdout=PIPE, text=True, check=False)
    for line in result.stdout.splitlines():
        if not (match := re.search(YAMLLINT_LINE_PATTERN, line)):
            raise ValueError(f'could not parse yamllint output line {line!r}')
        orig_file_name, line_offset, _ = headers[match['fname']]
        yield Error(
            match['level'],
            f"{match['message']} [yl:{match['code']}]",
            orig_file_name,
            int(match['line']) + line_offset,
            int(match['column']),
        )


# pylint: disable=too-many-ancestors
class TrackedLocationLoader(yaml.loader.SafeLoader):
    '''Load YAML documents while keeping track of keys' line and column.

    We need to override construct_sequence to track the location of list items,
    and construct_mapping to track the location of keys.

    See also: https://stackoverflow.com/q/13319067
    '''
    def construct_sequence(self, node, deep=False):
        sequence = super().construct_sequence(node, deep)
        sequence.append([item_node.start_mark for item_node in node.value])
        return sequence

    def construct_mapping(self, node, deep=False):
        mapping = super().construct_mapping(node, deep=deep)
        mapping['_locations'] = {
            # Keys aren't necessarily strings, so parse them in YAML.
            self.construct_object(key_node): key_node.start_mark
            for key_node, _ in node.value
        }
        return mapping

    @staticmethod
    def remove_trackers(data):
        '''Remove temporary location tracker items.

        Original file locations are tracked using special properties and list
        items and used for more informative error messages, but they should not
        be present for schema validation, for example.
        '''
        if isinstance(data, dict):
            return {key: TrackedLocationLoader.remove_trackers(value)
                    for key, value in data.items()
                    if key != '_locations'}
        if isinstance(data, list):
            return [TrackedLocationLoader.remove_trackers(value)
                    for value in data[:-1]]
        return data


def get_schema_for_file(file_name: str) -> dict:
    '''Construct a schema to validate the YAML header of the given file.'''
    def package_name_matches(field, value, error):
        basename = os.path.basename(file_name)
        if f'{value.lower()}.sh' != basename:
            error(field, f'must match the file name {basename!r} '
                  'case-insensitively, excluding the .sh')

    def is_valid_require(field, value, error):
        _, sep, arch_re = value.partition(':')
        if sep:
            try:
                re.compile(arch_re)
            except re.error as exc:
                error(field, f'invalid architecture regex after colon: {exc}')

    def environment_schema(allow_list_values=False):
        return {
            'type': 'dict',
            'keysrules': {
                'type': 'string',
                'regex': r'^[a-zA-Z_][a-zA-Z0-9_]+$',
            },
            'valuesrules': {
                'anyof': [
                    {'type': 'string'},
                    {'type': 'list', 'schema': {'type': 'string'}},
                ],
            } if allow_list_values else {'type': 'string'},
        }

    def version_string_ok(field, value, error):
        if not file_name.startswith('defaults-') and \
           '%(defaults_upper)s' in value:
            error(field, 'cannot use %(defaults_upper)s in non-default recipe')

    def is_valid_regex(field, value, error):
        try:
            re.compile(value)
        except re.error as exc:
            error(field, f'invalid regex: {exc}')

    def is_relative_toplevel_path(field, value, error):
        if value.startswith('/'):
            error(field, 'expecting a relative path')
        if '/' in value:
            error(field, 'expecting a toplevel path (i.e. without slashes)')

    requires = {
        'type': 'list',
        'schema': {
            'type': 'string',
            'check_with': is_valid_require,
        }
    }

    git_url = {
        'type': 'string',
        'regex': r'^(https?|git)://.*$',
    }

    environment = environment_schema(allow_list_values=False)
    path_environment = environment_schema(allow_list_values=True)

    # This contains most keys, and can be used in other packages' override:.
    override_package = {
        'version': {'type': 'string', 'check_with': version_string_ok},
        'tag': {'type': 'string'},
        'source': git_url,
        'write_repo': git_url,
        'requires': requires,
        'build_requires': requires,
        'env': environment,
        'valid_defaults': {'type': 'list', 'schema': {'type': 'string'}},
        'prepend_path': path_environment,
        'append_path': path_environment,
        'force_rebuild': {'type': 'boolean'},
        'incremental_recipe': {'type': 'string'},
        'prefer_system': {'type': 'string', 'check_with': is_valid_regex},
        'prefer_system_check': {'type': 'string'},
        'system_requirement': {
            'type': 'string',
            'check_with': is_valid_regex,
            'dependencies': ('system_requirement_check',),
        },
        'system_requirement_check': {
            'type': 'string',
            'dependencies': ('system_requirement',),
        },
        'system_requirement_missing': {
            'type': 'string',
            'dependencies': ('system_requirement',),
        },
        'relocate_paths': {
            'type': 'list',
            'schema': {
                'type': 'string',
                'check_with': is_relative_toplevel_path,
            },
        },
    }

    return {
        'package': {
            'required': True,
            'type': 'string',
            'check_with': package_name_matches,
        },
        **override_package,
        # At the top level, the version key is required.
        'version': {
            'required': True,
            'type': 'string',
            'check_with': version_string_ok,
        },
        'disable': {'type': 'list', 'schema': {'type': 'string'}},
        'overrides': {
            'type': 'dict',
            'keysrules': {'type': 'string'},
            'valuesrules': {'type': 'dict', 'schema': override_package},
        },
    }


def position_of_key(tagged_object: dict,
                    path: tuple[str | int, ...]) -> tuple[int, int]:
    '''Find the line and column numbers of the specified key.'''
    cur_object_parent = tagged_object
    for path_element in path[:-1]:
        try:
            cur_object_parent = cur_object_parent[path_element]
        except KeyError:
            print(path_element, cur_object_parent)
            raise
    if isinstance(cur_object_parent, dict):
        direct_parent = cur_object_parent['_locations']
    elif isinstance(cur_object_parent, list):
        direct_parent = cur_object_parent[-1]
    else:
        raise TypeError(cur_object_parent)
    try:
        mark = direct_parent[path[-1]]
        return mark.line + 1, mark.column + 1
    except KeyError:
        # The key is not present, but probably required.
        return 1, 0


def emit_validation_errors(error_tree: ValidationErrorTree,
                           tagged_validated_object: dict,
                           file_name: str,
                           path: ObjectPath = ()) -> Iterable[Error]:
    '''Parse any validation errors from a cerberus validator.'''
    if isinstance(error_tree, dict):
        for key, suberrors in error_tree.items():
            yield from emit_validation_errors(
                suberrors, tagged_validated_object, file_name, path + (key,),
            )
    elif isinstance(error_tree, list):
        for subtree in error_tree:
            yield from emit_validation_errors(subtree, tagged_validated_object,
                                              file_name, path)
    elif isinstance(error_tree, str):
        line, column = position_of_key(tagged_validated_object, path)
        dotted_subpath = '.'.join(map(str, path))
        yield Error('error', f'{dotted_subpath}: {error_tree} [ali:schema]',
                    file_name, line, column)
    else:
        raise TypeError(f'cannot handle {error_tree!r}')


def check_keys_order(data: dict[str, Any],
                     orig_file_name: str,
                     line_offset: int) -> Iterable[Error]:
    '''Produce errors relating to key order in the YAML header data.'''
    def make_error(message: str, key: str) -> Error:
        rel_line, column = position_of_key(data, (key,))
        return Error('error', f'{message} [ali:key-order]',
                     orig_file_name, rel_line + line_offset, column)

    keys = list(data.keys())
    if 'requires' in data and 'build_requires' in data and \
       keys.index('requires') > keys.index('build_requires'):
        for key in ('requires', 'build_requires'):
            yield make_error('requires must come before build_requires', key)
    if 'package' in data:
        if keys.index('package') != 0:
            yield make_error('package: must be the first key in the file',
                             'package')
        if 'version' in data and keys.index('version') != 1:
            yield make_error('version: must be the second key in the file '
                             '(after package)', 'version')
        if 'tag' in data and keys.index('tag') != 2:
            yield make_error('tag: must be the third key in the file '
                             '(after version)', 'tag')
    else:
        if 'version' in data and keys.index('version') != 0:
            yield make_error('version: must be the first key in the override '
                             'declaration', 'version')
        if 'tag' in data and keys.index('tag') != 1:
            yield make_error('tag: must be the second key in the override '
                             'declaration (after version)', 'tag')


def header_lint(headers: FileParts) -> Iterable[Error]:
    '''Apply alidist-specific linting rules to YAML headers.'''
    def make_error(message: str, code: str,
                   rel_line: int, column: int) -> Error:
        return Error('error', f'{message} [ali:{code}]',
                     orig_file_name, rel_line + line_offset, column)

    for header in headers.values():  # we don't need the temporary file
        orig_file_name, line_offset, yaml_text = header
        if not yaml_text:
            yield make_error('metadata not found or empty '
                             "(is the '\\n---\\n' separator present?)",
                             'empty', 1, 0)
            return

        # Parse the source YAML, keeping track of the locations of keys.
        try:
            tagged_data = yaml.load(io.BytesIO(yaml_text),
                                    TrackedLocationLoader)
        except MarkedYAMLError as exc:
            mark = exc.problem_mark
            yield make_error(f'parse error: {exc.problem}', 'parse',
                             1 if mark is None else mark.line,
                             0 if mark is None else mark.column)
            continue
        except YAMLError as exc:
            yield make_error(f'unknown error parsing YAML: {exc}',
                             'parse', 1, 0)
            continue

        # Run schema validation against the "clean" data, without source
        # location markers.
        pure_data = TrackedLocationLoader.remove_trackers(tagged_data)

        # Basic sanity check.
        if not isinstance(pure_data, dict):
            yield make_error('recipe metadata must be a dictionary '
                             f'(got a {type(pure_data)} instead)',
                             'toplevel-nondict', 1, 0)
            continue

        # Make sure values have the types that they should.
        validator = cerberus.Validator(get_schema_for_file(orig_file_name))
        if not validator.validate(pure_data):
            yield from emit_validation_errors(validator.errors, tagged_data,
                                              orig_file_name)

        # Make sure the order of the most important keys is correct.
        yield from check_keys_order(tagged_data, orig_file_name, line_offset)
        for tagged_override_data in tagged_data.get('overrides', {}).values():
            yield from check_keys_order(tagged_override_data,
                                        orig_file_name, line_offset)


def main(args: Namespace) -> int:
    '''Script entry point, returning the desired exit code.'''
    formatter = ERROR_FORMATTERS[args.format]
    progname = os.path.basename(sys.argv[0])
    have_error = False
    with tempfile.TemporaryDirectory(prefix=progname) as tempdir:
        headers, scripts = split_files(tempdir, args.recipes)
        errors = itertools.chain(
            () if args.no_lint else header_lint(headers),
            () if args.no_yamllint else yamllint(headers),
            () if args.no_shellcheck else shellcheck(scripts),
        )
        for error in errors:
            have_error |= error.level == 'error'
            print(formatter(error))
    return 1 if have_error else 0


def parse_args() -> Namespace:
    '''Parse and return command-line arguments.'''
    parser = ArgumentParser(description=__doc__)
    parser.add_argument('-S', '--no-shellcheck', action='store_true',
                        help="don't run shellcheck on the main script")
    parser.add_argument('-Y', '--no-yamllint', action='store_true',
                        help="don't run yamllint on the header")
    parser.add_argument('-L', '--no-lint', action='store_true',
                        help="don't run internal linter on the header")
    parser.add_argument('-f', '--format', metavar='FORMAT',
                        choices=ERROR_FORMATTERS.keys(), default='gcc',
                        help=('format of error messages '
                              '(one of %(choices)s; default %(default)s)'))
    parser.add_argument('recipes', metavar='RECIPE', nargs='+',
                        type=FileType('rb'),
                        help='a file name to check (use - for stdin)')
    return parser.parse_args()


if __name__ == '__main__':
    sys.exit(main(parse_args()))
